{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import pytz\n",
    "import datetime\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from text_unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models\n",
    "Define directory where the models are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_models = \"D:/Project data/Data Project Sentiment Race/02_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load vectorizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bow = load(dir_models+'vectorizer_bow.joblib')\n",
    "vectorizer_tfidf = load(dir_models+'vectorizer_tfidf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = load(dir_models+'naivebayes_tfidf.joblib') \n",
    "logistic = load(dir_models+'logistic_tfidf_cv.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function for dividing iterable into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(iterable, chunk_size):\n",
    "    size = iterable.shape[0]\n",
    "    if size < chunk_size:\n",
    "        yield iterable\n",
    "    chunks_nb = int(size / chunk_size)\n",
    "    iter_ints = range(0, chunks_nb)\n",
    "    for i in iter_ints:\n",
    "        j = i * chunk_size\n",
    "        if i + 1 < chunks_nb:\n",
    "            k = j + chunk_size\n",
    "            yield iterable[j:k]\n",
    "        else:\n",
    "            yield iterable[j:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that makes predictions with Naive Bayes by making the forecasts iteratively to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_GaussianNB(model, X, chunk_size):\n",
    "    y = np.empty(0)\n",
    "    for X_i in get_chunks(X, chunk_size):\n",
    "        y_i = model.predict(X_i.toarray())\n",
    "        y = np.concatenate((y, y_i))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "In this section the functions and the data necessary for processing the tweets are defined/loaded.\n",
    "\n",
    "## Data location\n",
    "Define directory where the data is loacated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_raw_data = \"D:/Project data/Data Project Sentiment Race/00_raw/\"\n",
    "dir_original_data = 'D:/Original data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data issues\n",
    "Load file that summarise the issues with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_issue = pd.read_csv(\n",
    "    dir_raw_data + 'data_issue_info.tsv', \n",
    "    delimiter = '\\t')\n",
    "to_be_excluded = data_issue.loc[data_issue['exclude']==1, 'rpid'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping file\n",
    "Load mapping file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_mapping = pd.read_csv(\n",
    "    dir_raw_data + \"SP500_Company_Mapping.tsv\",\n",
    "    delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowercase company's ticker and name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_mapping['taq_ticker'] = company_mapping['taq_ticker'].map(lambda ticker: ticker.lower())\n",
    "company_mapping['original_name'] = company_mapping['original_name'].map(lambda name: name.lower())\n",
    "company_mapping['cleaned_name'] = company_mapping['cleaned_name'].map(lambda name: name.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove observations for which we have data issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = company_mapping['rpid'].map(lambda x: x in to_be_excluded)\n",
    "company_mapping = company_mapping.loc[~to_remove, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons\n",
    "Load emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = pd.read_csv(dir_raw_data + 'emojis.csv', delimiter=';', index_col='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load tagged emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_tags = pd.read_csv(dir_raw_data + 'emojis_tags.csv', delimiter=';', index_col='unicode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define regular expressions for positive and negative emoticons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis_positive = '|'.join('(' + pd.concat([emojis_tags.loc[emojis_tags['tag'] == 'positive'], emojis],\n",
    "                                           join='inner', axis=1)['ftu8'] + ')')\n",
    "emojis_negative = '|'.join('(' + pd.concat([emojis_tags.loc[emojis_tags['tag'] == 'negative'], emojis],\n",
    "                                           join='inner', axis=1)['ftu8'] + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning function\n",
    "Define function that cleans the text of each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, regex_cashtag, regex_ticker, regex_name, regex_cleanname, regex_posemoji, regex_negemoji, lemmer):\n",
    "    # Transform text to unicode:\n",
    "    text = unidecode(text)\n",
    "    # Replace positive emojis:\n",
    "    text = re.sub(regex_posemoji, ' emojipostag ', text)\n",
    "    text = re.sub('(:[)])|(;[)])|(:-[)])|(=[)])|(:D)', ' emojipostag ', text)\n",
    "    # Replace negative emojis:\n",
    "    text = re.sub(regex_negemoji, ' emojinegtag ', text)\n",
    "    text = re.sub('(:[(])|(:-[(])|(=[(])', ' emojinegtag ', text)\n",
    "    # Remove other emojis:\n",
    "    text = re.sub('[<][a-z0-9]+[>]', ' ', text)\n",
    "    # Remove HTML tags:\n",
    "    cleanhtml = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    text = re.sub(cleanhtml, '', text)\n",
    "    # Change encoding to remove non-english words\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # Lower case all letters\n",
    "    text = text.lower()\n",
    "    # Remove \"'s\"\n",
    "    text = re.sub(r\"'s(?=\\s)\", ' ', text)\n",
    "    # Replace usernames with \"usernametag\"\n",
    "    text = re.sub(r'[@]\\w+(?=\\s|$)', ' usernametag ', text)\n",
    "    # Replace Twitter picuters with picturetag\n",
    "    text = re.sub(r'pic.twitter.com/[0-9a-zA-Z]*(?=\\s|$)', ' picturetag ', text)\n",
    "    # Replace URLs with \"urltag\"\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' urltag ', text)\n",
    "    # Replace Q1 with first quarter tag:\n",
    "    text = re.sub('q1', ' firstquartertag ', text)\n",
    "    # Replace Q2 with first quarter tag:\n",
    "    text = re.sub('q2', ' secondquartertag ', text)\n",
    "    # Replace Q3 with first quarter tag:\n",
    "    text = re.sub('q3', ' thirdquartertag ', text)\n",
    "    # Replace Q4 with first quarter tag:\n",
    "    text = re.sub('q4', ' fourthquartertag ', text)\n",
    "    # Replace percent numbers with tag:\n",
    "    text = re.sub(r'([+-]*\\d+[.,:]\\d+[%])|([+-]*\\d+[%])', ' numbertag ', text)\n",
    "    # Replace numbers with tag:\n",
    "    text = re.sub(r'([+-]*\\d+[.,:]\\d+)|([+-]*\\d+)', ' numbertag ', text)\n",
    "    # Replace company cashtag\n",
    "    text = re.sub(regex_cashtag, ' companycashtag ', text)\n",
    "    # Replace company ticker\n",
    "    text = re.sub(regex_ticker, ' companytickertag ', text)\n",
    "    # Replace all other cashtags with a tag\n",
    "    text = re.sub(r'[$]\\b[a-zA-z]+\\b', ' cashtag ', text)\n",
    "    # Replace company name with tag:\n",
    "    text = re.sub(regex_name, ' companynametag ', text)\n",
    "    text = re.sub(regex_cleanname, ' companynametag ', text)\n",
    "    # Characters that appear more two or more times are shortened (e.g. loooool -> lool):\n",
    "    text = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', text)\n",
    "    # Remove remaining punctuation\n",
    "    text = re.sub('['+string.punctuation+']', ' ', text)\n",
    "    # Remove double spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Lemmatize text:\n",
    "    text = ' '.join([lemmer.lemmatize(word) for word in text.split(' ')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the lemmatizer need for the cleaning function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = pd.read_csv(dir_original_data + 'Miscellaneous/NYSE_closing_days.tsv', delimiter='\\t')\n",
    "\n",
    "holidays.columns = ['Date', 'Time', 'Holiday']\n",
    "\n",
    "holidays = holidays.drop('Time', axis=1)\n",
    "\n",
    "holidays['Holiday'] = holidays['Holiday'].map(lambda x: x==1)\n",
    "holidays['Date'] = holidays['Date'].map(lambda x: pd.Timestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_info = pd.DataFrame({'Date': pd.date_range(start=datetime.datetime(2010, 1, 1), end=datetime.datetime(2019, 1, 1))})\n",
    "\n",
    "closing_info['Weekend'] = closing_info['Date'].map(lambda x: x.weekday() in [5,6])\n",
    "\n",
    "closing_info = closing_info.merge(holidays, how='left', on='Date')\n",
    "\n",
    "closing_info['Holiday'] = closing_info['Holiday'].fillna(False)\n",
    "\n",
    "closing_info['Closed'] = closing_info.apply(lambda x: x['Weekend'] or x['Holiday'], axis=1)\n",
    "\n",
    "closing_info.Date = closing_info.Date.dt.date\n",
    "\n",
    "closing_info = closing_info.drop(['Weekend', 'Holiday'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process messages\n",
    "Define location where the processed data should be saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_processed = \"D:/Project data/Data Project Sentiment Race/01_processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define bullishness measure for aggregating intra-day sentiment ([Antweiler and Frank, 2004](https://onlinelibrary.wiley.com/doi/10.1111/j.1540-6261.2004.00662.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bull(x):\n",
    "     return np.log((1+np.sum(x>0))/(1+np.sum(x<0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define whether the aggregation takes place close-to-close or open-to-open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_tweets = 'c2c'\n",
    "if aggregation_tweets == 'c2c':\n",
    "    hours_shift = 8\n",
    "    minutes_shift = 0\n",
    "elif aggregation_tweets == 'o2o':\n",
    "    hours_shift = -9\n",
    "    minutes_shift = -30\n",
    "else:\n",
    "    hours_shift = 0\n",
    "    minutes_shift = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering of the messages, i.e. which messages should we keep in the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only messages which contain the company's cashtag:\n",
    "has_cashtag = True\n",
    "# Keep only messages which only contain the compnay's cashtag:\n",
    "unique_cashtag = True\n",
    "# Unique cashtags makes only sense if we keep only tweets which mention the company's cashtag:\n",
    "unique_cashtag = unique_cashtag and has_cashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define name datails for saving the aggregated sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_specifications = ''\n",
    "\n",
    "file_specifications = file_specifications + aggregation_tweets\n",
    "\n",
    "if has_cashtag:\n",
    "    file_specifications = file_specifications + '_cashtag_only'\n",
    "if unique_cashtag:\n",
    "    file_specifications = file_specifications + '_unique'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StockTwits\n",
    "Load StockTwits messages, clean text, predict sentiment, aggregate sentiment on daily level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_stocktwits = pd.DataFrame()\n",
    "tz_NY = pytz.timezone('America/New_York')\n",
    "for rpid_i in company_mapping['rpid'].unique():\n",
    "    # Load data for the company with ID 'rpid_i':\n",
    "    data_i = pd.read_csv(\n",
    "        dir_original_data + 'StockTwits SP500/' + rpid_i + '_tweets.tsv',\n",
    "        encoding=\"ANSI\", quotechar='\"', delimiter=\"\\t\", engine='python')\n",
    "    # Keep only relevant columns:\n",
    "    data_i = data_i[['text', 'tweet_datetime']]\n",
    "    # Define regular expression for the company's cashtag:\n",
    "    cashtag_regex_i = '|'.join(r'([$]{1}\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'taq_ticker'] + r'\\b)')\n",
    "    ticker_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'taq_ticker'] + r'\\b)')\n",
    "    # Define regular expression for the company's name:\n",
    "    name_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'original_name'] + r'\\b)')\n",
    "    nameclean_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'cleaned_name'] + r'\\b)')\n",
    "    # Clean text data:\n",
    "    data_i['text'] = data_i['text'].map(lambda x: clean_text(x,\n",
    "                                                             cashtag_regex_i,\n",
    "                                                             ticker_regex_i,\n",
    "                                                             name_regex_i,\n",
    "                                                             nameclean_regex_i,\n",
    "                                                             emojis_positive,\n",
    "                                                             emojis_negative,\n",
    "                                                             lemmer))\n",
    "    # Count number of company cashtags:\n",
    "    data_i['num_companycashtag'] = data_i['text'].map(lambda x: len(re.findall(r'\\bcompanycashtag\\b', x))) \n",
    "    # Count number of other cashtags:\n",
    "    data_i['num_cashtag'] = data_i['text'].map(lambda x: len(re.findall(r'\\bcashtag\\b', x))) \n",
    "    # If wanted, remove tweets that do not mention the company's cashtag:\n",
    "    if has_cashtag:\n",
    "        data_i = data_i.loc[data_i['num_companycashtag']>0]\n",
    "    # If wanted, remove tweets that mention other cashtags:\n",
    "    if unique_cashtag:\n",
    "        data_i = data_i.loc[data_i['num_cashtag']==0]\n",
    "    # Tranform strings to timestamps:\n",
    "    data_i['tweet_datetime'] = data_i['tweet_datetime'].map(lambda x: pd.Timestamp(x))\n",
    "    # Change timezone to Eastern Time:\n",
    "    data_i['tweet_datetime_ET'] = data_i['tweet_datetime'].map(lambda x: x.astimezone(tz_NY))\n",
    "    # Shift time depending on the aggregation scheme choosen previously:\n",
    "    data_i['tweet_datetime_ET_shifted'] = data_i['tweet_datetime_ET'].map(lambda x: x + datetime.timedelta(hours=hours_shift, minutes=minutes_shift))\n",
    "    # Define date based on the shifted ET timestamp:\n",
    "    data_i['Date'] = data_i['tweet_datetime_ET_shifted'].dt.date\n",
    "    # Vectorize the text data:\n",
    "    X_i = vectorizer_bow.transform(data_i['text'])\n",
    "    X_i = vectorizer_tfidf.transform(X_i)\n",
    "    # Predict sentiemnt of the messages using the logistic and the naive bayes model:\n",
    "    data_i['Logistic'] = (logistic.predict(X_i) -0.5)*2\n",
    "    data_i['NaiveBayes'] = (predict_GaussianNB(naivebayes, X_i, 10000) -0.5)*2\n",
    "    # For the aggregation, we shift the date of messages posted during holidays or weekends to the next trading day:\n",
    "    data_i = data_i.merge(closing_info, how='left', on='Date')[['Date', 'Closed', 'Logistic', 'NaiveBayes']]\n",
    "    while any(data_i.Closed):\n",
    "        data_i['Date'] = data_i.apply(lambda x: x['Date'] + datetime.timedelta(days=1) if x['Closed'] else x['Date'], axis=1)\n",
    "        data_i = data_i.drop('Closed', axis=1).merge(closing_info, how='left', on='Date')\n",
    "    # Aggregate sentiments on a daily basis:\n",
    "    sentiment_i = data_i.drop('Closed', axis=1).groupby('Date').aggregate({'Logistic': [bull, np.mean], 'NaiveBayes': [bull, np.mean]} )\n",
    "    # Delete the raw data:\n",
    "    del(data_i)\n",
    "    # Transform multi-index column names to single level:\n",
    "    sentiment_i.columns = ['_'.join(col).strip() for col in sentiment_i.columns.values]\n",
    "    # Date (which acts as an index) to a column:\n",
    "    sentiment_i.reset_index(level=0, inplace=True)\n",
    "    # Add information about RavenPack ID:\n",
    "    sentiment_i['rpid'] = rpid_i\n",
    "    # Append data:\n",
    "    sentiment_stocktwits = sentiment_stocktwits.append(sentiment_i, ignore_index=True)\n",
    "    # Remove the sentiment data:\n",
    "    del(sentiment_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the StockTwits sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_stocktwits.to_csv(dir_processed + 'StockTwits_daily_' + file_specifications + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete data frame from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(sentiment_stocktwits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_twitter = pd.DataFrame()\n",
    "tz_NY = pytz.timezone('America/New_York')\n",
    "for rpid_i in company_mapping['rpid'].unique():\n",
    "    # Load data for the company with ID 'rpid_i':\n",
    "    data_i = pd.read_csv(\n",
    "        dir_original_data + 'Twitter SP500/' + rpid_i + '_tweets.tsv',\n",
    "        encoding=\"ANSI\", quotechar='\"', delimiter=\"\\t\", engine='python')\n",
    "    # Keep only relevant columns:\n",
    "    data_i = data_i[['text', 'datetime']]\n",
    "    # Define regular expression for the company's cashtag:\n",
    "    cashtag_regex_i = '|'.join(r'([$]{1}\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'taq_ticker'] + r'\\b)')\n",
    "    ticker_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'taq_ticker'] + r'\\b)')\n",
    "    # Define regular expression for the company's name:\n",
    "    name_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'original_name'] + r'\\b)')\n",
    "    nameclean_regex_i = '|'.join(r'(\\b' + company_mapping.loc[company_mapping['rpid'] == rpid_i, 'cleaned_name'] + r'\\b)')\n",
    "    # Clean text data:\n",
    "    data_i['text'] = data_i['text'].map(lambda x: clean_text(x,\n",
    "                                                         cashtag_regex_i,\n",
    "                                                         ticker_regex_i,\n",
    "                                                         name_regex_i,\n",
    "                                                         nameclean_regex_i,\n",
    "                                                         emojis_positive,\n",
    "                                                         emojis_negative,\n",
    "                                                         lemmer))\n",
    "    # Count number of company cashtags:\n",
    "    data_i['num_companycashtag'] = data_i['text'].map(lambda x: len(re.findall(r'\\bcompanycashtag\\b', x))) \n",
    "    # Count number of other cashtags:\n",
    "    data_i['num_cashtag'] = data_i['text'].map(lambda x: len(re.findall(r'\\bcashtag\\b', x))) \n",
    "    # If wanted, remove tweets that do not mention the company's cashtag:\n",
    "    if has_cashtag:\n",
    "        data_i = data_i.loc[data_i['num_companycashtag']>0]\n",
    "    # If wanted, remove tweets that mention other cashtags:\n",
    "    if unique_cashtag:\n",
    "        data_i = data_i.loc[data_i['num_cashtag']==0]\n",
    "    # Tranform strings to timestamps:\n",
    "    data_i['datetime'] = data_i['datetime'].map(lambda x: pd.Timestamp(x).tz_localize(tz='Europe/Zurich', ambiguous=True))\n",
    "    # Change timezone to Eastern Time:\n",
    "    data_i['datetime_ET'] = data_i['datetime'].map(lambda x: x.astimezone(tz_NY))\n",
    "    # Shift time depending on the aggregation scheme choosen previously:\n",
    "    data_i['datetime_ET_shifted'] = data_i['datetime_ET'].map(lambda x: x + datetime.timedelta(hours=hours_shift, minutes=minutes_shift))\n",
    "    # Define date based on the shifted ET timestamp:\n",
    "    data_i['Date'] = data_i['datetime_ET_shifted'].dt.date\n",
    "    # Vectorize the text data:\n",
    "    X_i = vectorizer_bow.transform(data_i['text'])\n",
    "    X_i = vectorizer_tfidf.transform(X_i)\n",
    "    # Predict sentiemnt of the messages using the logistic and the naive bayes model:\n",
    "    data_i['Logistic'] = (logistic.predict(X_i) -0.5)*2\n",
    "    data_i['NaiveBayes'] = (predict_GaussianNB(naivebayes, X_i, 10000) -0.5)*2\n",
    "    # For the aggregation, we shift the date of messages posted during holidays or weekends to the next trading day:\n",
    "    data_i = data_i.merge(closing_info, how='left', on='Date')[['Date', 'Closed', 'Logistic', 'NaiveBayes']]\n",
    "    while any(data_i.Closed):\n",
    "        data_i['Date'] = data_i.apply(lambda x: x['Date'] + datetime.timedelta(days=1) if x['Closed'] else x['Date'], axis=1)\n",
    "        data_i = data_i.drop('Closed', axis=1).merge(closing_info, how='left', on='Date')\n",
    "    # Aggregate sentiments on a daily basis:\n",
    "    sentiment_i = data_i.drop('Closed', axis=1).groupby('Date').aggregate({'Logistic': [bull, np.mean], 'NaiveBayes': [bull, np.mean]} )\n",
    "    # Transform multi-index column names to single level:\n",
    "    sentiment_i.columns = ['_'.join(col).strip() for col in sentiment_i.columns.values]\n",
    "    # Date (which acts as an index) to a column:\n",
    "    sentiment_i.reset_index(level=0, inplace=True)\n",
    "    # Add information about RavenPack ID:\n",
    "    sentiment_i['rpid'] = rpid_i\n",
    "    # Append data:\n",
    "    sentiment_twitter = sentiment_twitter.append(sentiment_i, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Twitter sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_twitter.to_csv(dir_processed + 'Twitter_daily_' + file_specifications + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete data frame from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(sentiment_twitter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
